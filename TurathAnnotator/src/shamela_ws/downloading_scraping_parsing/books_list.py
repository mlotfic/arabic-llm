# -*- coding: utf-8 -*-
"""
Created on Thu Jan 23 17:24:51 2025

@author: m.lotfi

"""

import datetime
import gzip
import json
import logging
import os
import re
import time
from copy import copy
from dataclasses import dataclass, field
from pathlib import Path
from typing import Dict, List, Optional, Union

import pandas as pd
import selenium
from bs4 import BeautifulSoup, Comment, NavigableString, Tag
from bson import ObjectId
from selenium import webdriver
from selenium.common.exceptions import TimeoutException, WebDriverException
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.support.ui import WebDriverWait
from webdriver_manager.chrome import ChromeDriverManager

# [custom import]
# ğŸ“Œ **data_extraction_tags**
from scraping.data_extraction_tags import (
    get_author,
    get_chapters_tree,
    get_hadith_base_info,
    get_hadith_info,
    get_hadith_info_thaskeel,
    get_hadith_narrators,
    get_page_navigation,
    get_page_volume,
    get_tkhreg_hadith_api,
    get_toc_path,
    get_narrator_data_from_tags,
)

# ğŸ“Œ **data_proccessing**
from scraping.data_proccessing import (
    bytes_to_human_readable,
    clean_html,
    clean_text,
    extract_number_before_haddathana,
    extract_text_from_html,
    find_common_columns,
    get_h_wa_haddathana_count_occurrences,
    get_url_components,
    remove_diacritics_and_spaces,
    remove_diacritics_spaces_and_normalize_numbers,
    divide_pages_into_parts,
)

# ğŸ“Œ **handles_files**
from scraping.handles_files import (
    convert_to_serializable,
    is_files_saved,
    save_html_files,
    to_csv_pd,
    to_html_append,
    to_json_csv,
    to_json_with_arabic,

)

# ğŸ“Œ **html_proccessing**
from scraping.html_proccessing import (
    parse_hadith_isnad,
    parse_hadith_page,
)

# ğŸ“Œ **time_date**
from scraping.time_date import (
    generate_random_delay_gauss,
)

# ğŸ“Œ **web_automation**
from scraping.web_automation import (
    create_human_like_browser,
    get_api_response,
    get_driver,
    handle_modal_interaction,
    is_modal_open,
    move_mouse_randomly,
    simulate_human_scroll,
    simulate_human_typing,
    try_close_modal_js,
    try_open_modal_js,
    try_open_modal,
    scrape_pages
)

# Configure custom logging if needed
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(levelname)s - %(message)s',
    filename='modal_interactions.log'
)

## **âœ… Setting initial variables**
wait_time = 10
timeout   = 10


from pprint import pprint

from urllib.parse import urlparse, parse_qs

def to_csv(df, csv_path):
    """Save DF to file with proper encoding for Arabic text"""
    # Ensure directory exists
    os.makedirs(os.path.dirname(csv_path), exist_ok=True)    
    df.to_csv(csv_path, index=False, encoding="utf-8-sig")
    return csv_path


def simple_extraction(url):
    parsed_url = urlparse(url)
    path_segments = parsed_url.path.split('/')
    
    # Find book_id
    book_id = next((seg for seg in path_segments if seg.isdigit()), None)
    
    # Parse query parameters
    query_params = parse_qs(parsed_url.query)
    
    return book_id, query_params.get('idfrom', [None])[0], query_params.get('idto', [None])[0]

# More comprehensive version with additional patterns
def advanced_date_standardization(date_str):
    """
    Advanced date standardization with multiple patterns
    """
    if not date_str or date_str.strip() == '':
        return "", {
            'hijri_year': "",
            'gregorian_year': "",
            'hijri': "",
            'gregorian': ""
        }
    
    # Clean input
    date_str = date_str.strip(' -')
    
    # Various date patterns
    patterns = {
        'hijri': [
            r'(\d{4})\s*Ù‡Ù€',
            r'(\d{4})\s*Ù‡Ù€Ù€',
            r'(\d{4})\s*Ù‡Ù€Ù€Ù€',
            r'(\d{4})\s*Ù‡'
        ],
        'gregorian': [
            r'(\d{4})\s*Ù…',
            r'(\d{4})\s*Ù…ÙŠÙ„Ø§Ø¯ÙŠ',
            r'(\d{4})'
        ]
    }
    
    try:
        # Find Hijri year
        hijri_year = None
        for pattern in patterns['hijri']:
            match = re.search(pattern, date_str)
            if match:
                hijri_year = match.group(1)
                break
        
        # Find Gregorian year
        gregorian_year = None
        for pattern in patterns['gregorian']:
            match = re.search(pattern, date_str)
            if match:
                gregorian_year = match.group(1)
                break
        
        # Format dates
        hijri = f"{hijri_year} Ù‡Ù€" if hijri_year else ""
        gregorian = f"{gregorian_year} Ù…" if gregorian_year else ""
        
        # Create standardized format
        if hijri and gregorian:
            standardized_date = f"{gregorian} - {hijri}"
        else:
            standardized_date = hijri or gregorian
        
        return  standardized_date, {
            'hijri_year'        : int(hijri_year) if hijri_year else None,
            'gregorian_year'    : int(gregorian_year) if gregorian_year else None,
            'hijri'             : hijri,
            'gregorian'         : gregorian
        }
        
    except Exception as e:
        print(f"Error in advanced date processing: {date_str}, Error: {e}")
        return "", {
            'hijri_year': "",
            'gregorian_year': "",
            'hijri': "",
            'gregorian': ""
        }
# subject_info = subjects[0]
def extract_book_details(raw_html, subject_info, url):
    soup = BeautifulSoup(raw_html, 'html.parser')
    book_list = []

    # Find the UL with book items
    div = soup.find('div', id="cat_books")
    if div:
        # Find all list items
        li_elements = div.find_all('div', class_="book_item margin-top-10")
        len(li_elements)
        
        # li_element = li_elements[0]
    
        for li_element in li_elements:
            result = {}
            book_meta = li_element.find("a", class_="book_title text-primary")
            if book_meta :
                result['book_title'] = book_meta.text.strip()
                result['book_url']   = book_meta.get("href", "")
                result['book_id']    = result['book_url'].split("/")[-1] if result['book_url'] else ""
                
            author_meta = li_element.find("a", class_="text-gray")
            if author_meta :
                result['author']      = author_meta.text.strip()
                result['author_url']   = author_meta.get("href", "")
                result['author_id']    = result['author_url'].split("/")[-1] if result['author_url'] else ""
            
            p = li_element.find("p").text
            
            lines = p.strip().split('\n')
            
            for line in lines:
                if line.startswith("Ø§Ù„ÙƒØªØ§Ø¨:"):
                    match = re.match(r'Ø§Ù„ÙƒØªØ§Ø¨:\s*([^\(]+)\s*\((.+)\)', line)
                    if match:
                        result['p_title'] = match.group(1).strip()
                        result['p_title_note'] = match.group(2).strip()
                    else:
                        result['p_title'] = line[len("Ø§Ù„ÙƒØªØ§Ø¨:"):].strip()
                elif line.startswith("Ø§Ù„Ù…Ø¤Ù„Ù:"):
                    match = re.match(r'Ø§Ù„Ù…Ø¤Ù„Ù:\s*([^\(]+)\s*\((.+)\)', line)
                    if match:
                        result['p_author'] = match.group(1).strip()
                        result['p_author_died'] = match.group(2).strip()
                    else:
                        result['p_author'] = line[len("Ø§Ù„Ù…Ø¤Ù„Ù:"):].strip()
                elif line.startswith("Ø§Ù„Ù…Ø­Ù‚Ù‚:") or line.startswith("ØªØ­Ù‚ÙŠÙ‚:"):
                    result['p_verified_by'] = line.split(":", 1)[1].strip()
                elif line.startswith("Ø£ØµÙ„ Ø§Ù„ÙƒØªØ§Ø¨:"):
                    result['book_origin'] = line.split(":", 1)[1].strip()
                elif line.startswith("Ø·Ø¨Ø¹ Ø¹Ù„Ù‰ Ù†ÙÙ‚Ø©:"):
                    result['sponsored_by'] = line.split(":", 1)[1].strip()
                elif line.startswith("Ø§Ù„Ù†Ø§Ø´Ø±:"):
                    result['p_publisher'] = line[len("Ø§Ù„Ù†Ø§Ø´Ø±:"):].strip()
                elif line.startswith("Ø§Ù„Ø·Ø¨Ø¹Ø©:"):
                    match = re.match(r'Ø§Ù„Ø·Ø¨Ø¹Ø©:\s*([^\ØŒ,]+)[ØŒ,]\s*(.+)', line)
                    if match:
                        result['p_edition'] = match.group(1).strip()
                        result['p_published'] = match.group(2).strip()
                    else:
                        result['p_edition'] = line[len("Ø§Ù„Ø·Ø¨Ø¹Ø©:"):].strip()
                
                elif line.startswith("Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø¬Ø²Ø§Ø¡:"):
                    result['p_volumes'] = line[len("Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø¬Ø²Ø§Ø¡:"):].strip()
                elif line.startswith("Ø¨Ø¥Ø´Ø±Ø§Ù:"):
                    result['supervisor'] = line.split(":", 1)[1].strip()
                elif line.startswith("Ø±Ø³Ø§Ù„Ø©:") or line.startswith("Ø£Ø·Ø±ÙˆØ­Ø©:"):
                    result['thesis_type'] = line.split(":", 1)[1].strip()
                elif line.startswith("Ù‚Ø¯Ù… Ù„Ù‡:"):
                    result['preface_by'] = line.split(":", 1)[1].strip()
                elif line.startswith("Ø£Ø·Ø±ÙˆØ­Ø©:"):
                    rest = line.split(":", 1)[1].strip()
                    # Try to extract supervisor and date
                    parts = rest.split("ØŒ")
                    for part in parts:
                        part = part.strip()
                        if part.startswith("Ø¨Ø¥Ø´Ø±Ø§Ù"):
                            result['supervisor'] = part.replace("Ø¨Ø¥Ø´Ø±Ø§Ù", "").strip(" :ØŒ")
                        elif re.search(r'\d{3,4}\s*Ù‡Ù€', part):  # e.g., Ù¡Ù¤Ù¤Ù£ Ù‡Ù€
                            result['published_years'] = part.strip()
                        else:
                            result['thesis_type'] = part.strip()
                elif re.match(r'Ø±Ø§Ø¬Ø¹Ù‡(?:\s+ÙˆØ¯Ù‚Ù‚Ù‡)?\s*[:/ØŒ]?', line):
                    reviewer = re.sub(r'^Ø±Ø§Ø¬Ø¹Ù‡(?:\s+ÙˆØ¯Ù‚Ù‚Ù‡)?\s*[:/ØŒ]?', '', line).strip()
                    result['reviewer'] = reviewer
                elif line.startswith("Ø¹Ø¯Ø¯ Ø§Ù„ØµÙØ­Ø§Øª:"):
                    result['p_pages'] = line[len("Ø¹Ø¯Ø¯ Ø§Ù„ØµÙØ­Ø§Øª:"):].strip()
                elif line.startswith("[") and line.endswith("]"):
                    result['p_note'] = line[1:-1].strip()
            result['book_dtails'] = p
            
            result['category'] = subject_info['category']
            result['category_id'] = subject_info['category_id']
            result['category_book_count'] = subject_info['book_count']
            result['category_url'] = url
            
            print(result['book_dtails'])
            
            print('\n -------------------------------')
            
            # pprint(result)
            book_list.append(result)
        
    return book_list


# Create dictionary of subjects
subjects = [
    {
        "category": "Ø§Ù„Ø¹Ù‚ÙŠØ¯Ø©",
        "book_count": 803,
        "category_id": 1
    },
    {
        "category": "Ø§Ù„ÙØ±Ù‚ ÙˆØ§Ù„Ø±Ø¯ÙˆØ¯",
        "book_count": 151,
        "category_id": 2
    },
    {
        "category": "Ø§Ù„ØªÙØ³ÙŠØ±",
        "book_count": 271,
        "category_id": 3
    },
    {
        "category": "Ø¹Ù„ÙˆÙ… Ø§Ù„Ù‚Ø±Ø¢Ù† ÙˆØ£ØµÙˆÙ„ Ø§Ù„ØªÙØ³ÙŠØ±",
        "book_count": 309,
        "category_id": 4
    },
    {
        "category": "Ø§Ù„ØªØ¬ÙˆÙŠØ¯ ÙˆØ§Ù„Ù‚Ø±Ø§Ø¡Ø§Øª",
        "book_count": 151,
        "category_id": 5
    },
    {
        "category": "ÙƒØªØ¨ Ø§Ù„Ø³Ù†Ø©",
        "book_count": 1240,
        "category_id": 6
    },
    {
        "category": "Ø´Ø±ÙˆØ­ Ø§Ù„Ø­Ø¯ÙŠØ«",
        "book_count": 264,
        "category_id": 7
    },
    {
        "category": "Ø§Ù„ØªØ®Ø±ÙŠØ¬ ÙˆØ§Ù„Ø£Ø·Ø±Ø§Ù",
        "book_count": 128,
        "category_id": 8
    },
    {
        "category": "Ø§Ù„Ø¹Ù„Ù„ ÙˆØ§Ù„Ø³Ø¤Ù„Ø§Øª Ø§Ù„Ø­Ø¯ÙŠØ«ÙŠØ©",
        "book_count": 76,
        "category_id": 9
    },
    {
        "category": "Ø¹Ù„ÙˆÙ… Ø§Ù„Ø­Ø¯ÙŠØ«",
        "book_count": 320,
        "category_id": 10
    },
    {
        "category": "Ø£ØµÙˆÙ„ Ø§Ù„ÙÙ‚Ù‡",
        "book_count": 247,
        "category_id": 11
    },
    {
        "category": "Ø¹Ù„ÙˆÙ… Ø§Ù„ÙÙ‚Ù‡ ÙˆØ§Ù„Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„ÙÙ‚Ù‡ÙŠØ©",
        "book_count": 57,
        "category_id": 12
    },
    {
        "category": "Ø§Ù„Ù…Ù†Ø·Ù‚",
        "book_count": 11,
        "category_id": 13
    },
    {
        "category": "Ø§Ù„ÙÙ‚Ù‡ Ø§Ù„Ø­Ù†ÙÙŠ",
        "book_count": 85,
        "category_id": 14
    },
    {
        "category": "Ø§Ù„ÙÙ‚Ù‡ Ø§Ù„Ù…Ø§Ù„ÙƒÙŠ",
        "book_count": 86,
        "category_id": 15
    },
    {
        "category": "Ø§Ù„ÙÙ‚Ù‡ Ø§Ù„Ø´Ø§ÙØ¹ÙŠ",
        "book_count": 87,
        "category_id": 16
    },
    {
        "category": "Ø§Ù„ÙÙ‚Ù‡ Ø§Ù„Ø­Ù†Ø¨Ù„ÙŠ",
        "book_count": 151,
        "category_id": 17
    },
    {
        "category": "Ø§Ù„ÙÙ‚Ù‡ Ø§Ù„Ø¹Ø§Ù…",
        "book_count": 206,
        "category_id": 18,
    },
    {
        "category": "Ù…Ø³Ø§Ø¦Ù„ ÙÙ‚Ù‡ÙŠØ©",
        "book_count": 424,
        "category_id": 19
    },
    {
        "category": "Ø§Ù„Ø³ÙŠØ§Ø³Ø© Ø§Ù„Ø´Ø±Ø¹ÙŠØ© ÙˆØ§Ù„Ù‚Ø¶Ø§Ø¡",
        "book_count": 100,
        "category_id": 20
    },
    {
        "category": "Ø§Ù„ÙØ±Ø§Ø¦Ø¶ ÙˆØ§Ù„ÙˆØµØ§ÙŠØ§",
        "book_count": 28,
        "category_id": 21
    },
    {
        "category": "Ø§Ù„ÙØªØ§ÙˆÙ‰",
        "book_count": 64,
        "category_id": 22
    },
    {
        "category": "Ø§Ù„Ø±Ù‚Ø§Ø¦Ù‚ ÙˆØ§Ù„Ø¢Ø¯Ø§Ø¨ ÙˆØ§Ù„Ø£Ø°ÙƒØ§Ø±",
        "book_count": 623,
        "category_id": 23
    },
    {
        "category": "Ø§Ù„Ø³ÙŠØ±Ø© Ø§Ù„Ù†Ø¨ÙˆÙŠØ©",
        "book_count": 187,
        "category_id": 24
    },
    {
        "category": "Ø§Ù„ØªØ§Ø±ÙŠØ®",
        "book_count": 202,
        "category_id": 25
    },
    {
        "category": "Ø§Ù„ØªØ±Ø§Ø¬Ù… ÙˆØ§Ù„Ø·Ø¨Ù‚Ø§Øª",
        "book_count": 574,
        "category_id": 26
    },
    {
        "category": "Ø§Ù„Ø£Ù†Ø³Ø§Ø¨",
        "book_count": 52,
        "category_id": 27
    },
    {
        "category": "Ø§Ù„Ø¨Ù„Ø¯Ø§Ù† ÙˆØ§Ù„Ø±Ø­Ù„Ø§Øª",
        "book_count": 93,
        "category_id": 28
    },
    {
        "category": "ÙƒØªØ¨ Ø§Ù„Ù„ØºØ©",
        "book_count": 79,
        "category_id": 29
    },
    {
        "category": "Ø§Ù„ØºØ±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ø§Ø¬Ù…",
        "book_count": 134,
        "category_id": 30
    },
    {
        "category": "Ø§Ù„Ù†Ø­Ùˆ ÙˆØ§Ù„ØµØ±Ù",
        "book_count": 213,
        "category_id": 31
    },
    {
        "category": "Ø§Ù„Ø£Ø¯Ø¨",
        "book_count": 406,
        "category_id": 32
    },
    {
        "category": "Ø§Ù„Ø¹Ø±ÙˆØ¶ ÙˆØ§Ù„Ù‚ÙˆØ§ÙÙŠ",
        "book_count": 9,
        "category_id": 33
    },
    {
        "category": "Ø§Ù„Ø´Ø¹Ø± ÙˆØ¯ÙˆØ§ÙˆÙŠÙ†Ù‡",
        "book_count": 25,
        "category_id": 34
    },
    {
        "category": "Ø§Ù„Ø¨Ù„Ø§ØºØ©",
        "book_count": 44, 
        "category_id": 35
    },
    {
        "category": "Ø§Ù„Ø¬ÙˆØ§Ù…Ø¹",
        "book_count": 136,
        "category_id": 36
    },
    {
        "category": "ÙÙ‡Ø§Ø±Ø³ Ø§Ù„ÙƒØªØ¨ ÙˆØ§Ù„Ø£Ø¯Ù„Ø©",
        "book_count": 101,
        "category_id": 37
    },
    {
        "category": "Ø§Ù„Ø·Ø¨",
        "book_count": 14,
        "category_id": 38
    },
    {
        "category": "ÙƒØªØ¨ Ø¹Ø§Ù…Ø©",
        "book_count": 356,
        "category_id": 39
    },
    {
        "category": "Ø¹Ù„ÙˆÙ… Ø£Ø®Ø±Ù‰",
        "book_count": 26,
        "category_id": 40
    }
]


# Function to fetch and process URLs
def process_subjects(subjects):   
    
    all_books = []
    
    # âœ… Initialize browser
    driver = create_human_like_browser()
    
    for subject_info in subjects:
        # Get URL
        url = f"https://shamela.ws/category/{subject_info['category_id']}"
        category_id = subject_info['category_id']
        
        print(f"Processing {subject_info['category']} (ID: {category_id})")
        
        # âœ… Open target website
        driver.get(url)
        
        # Get the raw HTML
        raw_html = driver.page_source
        
        # âœ… Save Save clean html
        with open(f'./shamela/raw_html/cat{category_id}.html', "w", encoding="utf-8") as file:
            file.write(raw_html)
        
        # Polite delay between requests
        time.sleep(2)         

    # âœ… Close browser
    driver.quit()    
    
    # return all_books        

if __name__ == "__main__":
    
    # process_subjects(subjects)
    # # âœ… Close browser
    # driver.quit()
     
    
    all_books = []
    # set(book_df["publication_date"])
    # category_id = 1
    for subject_info in subjects:
        # Get URL
        url = f"https://shamela.ws/category/{subject_info['category_id']}"
        category_id = subject_info['category_id']
        
        
        print(f"Processing {subject_info['category']} (ID: {category_id})")
        
        
        with open(f'./shamela/raw_html/cat{category_id}.html', "r", encoding="utf-8") as file:
            raw_html = file.read()
            
        # Process books (using your existing book processing logic)
        books = extract_book_details(raw_html, subject_info, url)
        
        all_books = all_books + books
        
    # book_list = process_subjects(subjects)
    # book_list = process_subjects()
    book_df = pd.DataFrame(all_books) 
    to_csv(book_df, "./shamela/shamela_book_subject.csv")
    


